{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f075bf",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento del texto (`preprocess.py`)\n",
    "\n",
    "El preprocesamiento del texto se realiza en el archivo preprocess.py, que toma como entrada un archivo de texto (por ejemplo, un chat de WhatsApp) y lo transforma en un formato que el modelo puede usar. Primero, se lee el archivo línea por línea, se limpian caracteres poco útiles (como ciertos emojis o símbolos extraños) y, mediante expresiones regulares, se separa cada línea en componentes como fecha, contacto y mensaje, de manera que quede claro quién habló y qué dijo.\n",
    "\n",
    "Después, se aplica una tokenización personalizada: el texto se descompone en unidades más pequeñas (tokens) como palabras, números o signos de puntuación, respetando algunos tokens especiales (por ejemplo, nombres de contacto o el marcador <END>). A partir de todos los tokens que aparecen en el corpus se construye un vocabulario ordenado, asignando a cada token un índice entero; luego, cada token del texto se reemplaza por su índice correspondiente, obteniendo un gran vector de números.\n",
    "\n",
    "Por último, este vector se divide en dos partes: una porción grande se destina al conjunto de entrenamiento y una parte más pequeña al conjunto de validación. Ambos se guardan como tensores de PyTorch (por ejemplo train.pt y valid.pt), junto con el vocabulario, de modo que el modelo pueda trabajar directamente con estos datos numéricos en las siguientes etapas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf981c37",
   "metadata": {},
   "source": [
    "## 2. Tokenización, vocabulario y codificación (`utils.py`)\n",
    "\n",
    "El archivo `utils.py` contiene funciones auxiliares que relacionan texto con índices numéricos.\n",
    "\n",
    "Las funciones más importantes son:\n",
    "\n",
    "- `custom_tokenizer(text, spec_tokens, pattern)`  \n",
    "  Aplica una expresión regular que separa el texto en tokens, pero respeta ciertos tokens\n",
    "  especiales (`spec_tokens`) que no deben partirse. Esto es útil para manejar etiquetas\n",
    "  como contactos o marcadores de fin de mensaje.\n",
    "\n",
    "- `get_vocab(text)`  \n",
    "  Recorre todo el texto tokenizado y construye el **conjunto de tokens únicos**. Este\n",
    "  conjunto se ordena y se usa como vocabulario.\n",
    "\n",
    "- `encode(tokens, vocab)`  \n",
    "  Toma una lista de tokens y un vocabulario, y devuelve una lista de índices enteros\n",
    "  (por ejemplo `[15, 8, 203, 5, ...]`). Si un token no está en el vocabulario se reemplaza\n",
    "  por `<UNK>`.\n",
    "\n",
    "- `decode(indices, vocab)`  \n",
    "  Realiza la operación inversa: dada una secuencia de índices, reconstruye la secuencia\n",
    "  de tokens. Esto se usa tanto para depuración como para mostrar el texto generado por el modelo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cfa1e0",
   "metadata": {},
   "source": [
    "## 3. Construcción de batches autoregresivos\n",
    "\n",
    "La construcción de los batches autoregresivos es fundamental para entrenar un modelo que aprenda a predecir el siguiente token en una secuencia. Primero, se toma el texto ya codificado como números y se divide en ventanas de tamaño fijo llamadas block_size. Cada ventana se utiliza de dos formas: como entrada (x), y como salida desplazada un paso hacia adelante (y). Por ejemplo, si el texto es [10, 20, 30, 40, 50] y el tamaño de ventana es 4, entonces x será [10, 20, 30, 40] y y será [20, 30, 40, 50].\n",
    "\n",
    "Luego, la función que arma los batches elige varias posiciones dentro del texto y extrae desde allí estas ventanas de manera simultánea. Para cada posición seleccionada, se guarda una ventana como x y la misma ventana desplazada un token como y, formando así los pares con los que el modelo aprenderá. Esta extracción se hace cada vez que se necesita un batch, de modo que el modelo ve múltiples fragmentos distintos del corpus.\n",
    "\n",
    "Finalmente, estos pares se envían al modelo durante el entrenamiento, permitiéndole aprender a predecir el siguiente token en cada posición. Este proceso repetido muchas veces garantiza que el modelo recorra completo en distintos contextos y aprenda la estructura del lenguaje de forma progresiva.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7396fbbc",
   "metadata": {},
   "source": [
    "## 4. Arquitectura del modelo GPT (explicada con el código)\n",
    "\n",
    "El modelo está basado en un Transformer tipo decodificador. En términos simples, su función es tomar una secuencia de tokens y aprender a predecir el siguiente. Todo esto se implementa en `model.py`, donde cada clase corresponde a una parte fundamental del modelo. A continuación explico las piezas principales con un lenguaje cercano y citando partes del código donde es necesario.\n",
    "\n",
    "\n",
    "\n",
    "### 4.1 Atención de una sola cabeza\n",
    "\n",
    "La clase `Head` implementa la atención. El objetivo es que cada token decida cuánto debe “mirar” a los tokens anteriores. En el código se crean las transformaciones de query, key y value:\n",
    "\n",
    "```python\n",
    "self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "```\n",
    "\n",
    "Luego, en el `forward`, se calcula la atención con:\n",
    "\n",
    "```python\n",
    "wei = q @ k.transpose(-2,-1)\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "```\n",
    "\n",
    "Con esto, el modelo mide similitudes entre tokens y usa la máscara para no acceder al futuro. Finalmente combina los valores `v` según estos pesos. Esta parte permite que el modelo elija qué información previa es más útil.\n",
    "\n",
    "\n",
    "\n",
    "### 4.2 Atención multi-cabeza\n",
    "\n",
    "La clase `MultiHeadAttention` simplemente ejecuta varias cabezas como la anterior en paralelo:\n",
    "\n",
    "```python\n",
    "heads_list = [Head(head_size) for _ in range(n_heads)]\n",
    "self.heads = nn.ModuleList(heads_list)\n",
    "```\n",
    "\n",
    "Cada cabeza aprende un tipo distinto de relación dentro del texto. Luego, las salidas se concatenan y se mezclan:\n",
    "\n",
    "```python\n",
    "out = torch.cat(heads_list, dim=-1)\n",
    "out = self.linear(out)\n",
    "```\n",
    "\n",
    "Esto ayuda al modelo a entender patrones más variados y complejos.\n",
    "\n",
    "\n",
    "\n",
    "### 4.3 Feed-forward por posición\n",
    "\n",
    "Después de la atención, cada token pasa por una red totalmente conectada definida en `FeedFoward`:\n",
    "\n",
    "```python\n",
    "self.net = nn.Sequential(\n",
    "    nn.Linear(embed_size, 4 * embed_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4 * embed_size, embed_size),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "```\n",
    "\n",
    "Esta red no mezcla información entre posiciones; transforma cada vector por separado. Sirve para refinar la información que salió de las cabezas de atención.\n",
    "\n",
    "\n",
    "\n",
    "### 4.4 Bloque Transformer completo\n",
    "\n",
    "El bloque completo combina atención, MLP, normalización y conexiones residuales. En el `forward` vemos:\n",
    "\n",
    "```python\n",
    "x = x + self.sa(self.ln1(x))\n",
    "x = x + self.ffwd(self.ln2(x))\n",
    "```\n",
    "\n",
    "Este patrón permite que el modelo agregue nueva información sin perder la representación original, haciendo que el entrenamiento sea más estable y permitiendo apilar varios bloques.\n",
    "\n",
    "\n",
    "\n",
    "### 4.5 Modelo completo GPTLanguageModel\n",
    "\n",
    "La clase `GPTLanguageModel` integra todo. Primero crea embeddings:\n",
    "\n",
    "```python\n",
    "self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "self.pos_embedding = nn.Embedding(block_size, embed_size)\n",
    "```\n",
    "\n",
    "Cada token y cada posición tienen su propio vector. Luego se combinan:\n",
    "\n",
    "```python\n",
    "x = tok_emb + pos_emb\n",
    "x = self.blocks(x)\n",
    "logits = self.linear_output(x)\n",
    "```\n",
    "\n",
    "Los logits finales indican qué token es más probable como siguiente. Si se entregan `targets`, se calcula la pérdida usando entropía cruzada para guiar el aprendizaje.\n",
    "\n",
    "El método `generate` muestra cómo el modelo escribe texto: predice probabilidades con softmax, elige un token mediante sampling con:\n",
    "\n",
    "```python\n",
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "\n",
    "y repite el proceso hasta encontrar el token de fin. De esta forma, el modelo va generando texto paso a paso.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c33e59",
   "metadata": {},
   "source": [
    "## 5. Función de pérdida y entrenamiento (`train.py`)\n",
    "\n",
    "### 5.1 Pérdida: entropía cruzada\n",
    "\n",
    "En el método `forward` del modelo, cuando se entregan `targets`, se realiza:\n",
    "\n",
    "1. Se reacomodan los logits de forma `(batch_size * time_steps, vocab_size)`.\n",
    "2. Se reacomodan los targets de forma `(batch_size * time_steps,)`.\n",
    "3. Se aplica `F.cross_entropy(logits, targets)`.\n",
    "\n",
    "Esta función compara la distribución de probabilidades que el modelo asigna al siguiente token\n",
    "(con una softmax implícita) con el token real observado en el corpus. Mientras más lejos estén,\n",
    "mayor será la pérdida; el entrenamiento busca minimizar esta cantidad.\n",
    "\n",
    "### 5.2 Bucle de entrenamiento\n",
    "\n",
    "En `train.py` se hace lo siguiente (en pseudocódigo):\n",
    "\n",
    "1. Cargar los tensores `train.pt` y `valid.pt` y el vocabulario.\n",
    "2. Crear el modelo `GPTLanguageModel` con los hiperparámetros de `config.py`.\n",
    "3. Definir el optimizador `AdamW` con una tasa de aprendizaje y un peso de decaimiento.\n",
    "4. Para un número fijo de iteraciones (`max_iters`):\n",
    "   - Obtener un batch de `x, y` desde `train` mediante `get_batch`.\n",
    "   - Poner el modelo en modo entrenamiento (`model.train()`).\n",
    "   - Calcular `logits` y `loss = model(x, y)`.\n",
    "   - Hacer `optimizer.zero_grad()`, `loss.backward()` y `optimizer.step()`.\n",
    "   - Periódicamente, evaluar el modelo en `train` y `valid` usando `estimate_loss`,\n",
    "     que pone el modelo en modo evaluación (`model.eval()`) y promedia la pérdida\n",
    "     sobre varios batches.\n",
    "\n",
    "5. Guardar el modelo entrenado en `assets/models/model.pt` para usarlo después.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149ca22",
   "metadata": {},
   "source": [
    "## 6. Generación de texto (modo chat, `model.generate` y `chat.py`)\n",
    "\n",
    "El archivo model.py implementa un Transformer de tipo decodificador, que es el núcleo del modelo GPT. La clase Head define la atención de una sola cabeza, donde cada token del contexto decide qué tanto debe mirar a los tokens anteriores. Esto se realiza generando matrices query, key y value a partir de la entrada, calculando similitudes con q @ k.transpose(-2,-1) y aplicando una máscara para impedir acceder al futuro. Esta operación permite que el modelo identifique qué partes de la secuencia previa son relevantes para entender la posición actual.\n",
    "\n",
    "Luego se define la clase MultiHeadAttention, que ejecuta varias cabezas de atención en paralelo para capturar distintos patrones del texto al mismo tiempo. Cada cabeza aprende relaciones diferentes y sus salidas se concatenan y mezclan mediante una capa lineal. Después aparece la clase FeedFoward, que aplica una pequeña red neuronal posición por posición para transformar la información obtenida por la atención. Estas dos partes, junto con normalización y conexiones residuales, se combinan dentro de la clase Block, donde se observa el patrón x = x + self.sa(...) y x = x + self.ffwd(...), lo que ayuda al modelo a estabilizar el entrenamiento y mantener información útil a lo largo de la red.\n",
    "\n",
    "Finalmente, la clase GPTLanguageModel une todos estos componentes. Primero crea embeddings para representar tokens y posiciones, los suma para formar la entrada inicial y los pasa por varios bloques Transformer definidos en self.blocks. Luego una capa lineal final produce los logits que permiten predecir el siguiente token. Si se entregan objetivos, se calcula la pérdida con entropía cruzada; y en el modo de generación, el modelo toma el último logit, aplica softmax y selecciona un token con torch.multinomial, repitiendo este proceso hasta encontrar el token de fin. Esto permite que el modelo genere texto autoregresivamente, construyendo una secuencia palabra por palabra.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
